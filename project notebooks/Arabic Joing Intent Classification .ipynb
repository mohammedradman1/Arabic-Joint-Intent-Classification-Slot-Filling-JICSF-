{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data=[]\n",
    "i_data=[]\n",
    "\n",
    "# loading NER annotations made through OpenAI API\n",
    "with open(\"./batch_output.jsonl\",\"r\",encoding=\"utf-8\") as lines:\n",
    "    for line in lines:\n",
    "        e_data.append(json.loads(line)) \n",
    "\n",
    "# loading NADA dataset\n",
    "intent_data=pd.read_csv(\"./data.csv\")\n",
    "\n",
    "# sorting NER labels based on the ID to match the NADA dataset\n",
    "entity_data = sorted(e_data, key=lambda x: int(x['custom_id'].split('-')[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_entities(text, entities):\n",
    "    '''\n",
    "    a function to match the NER labels with the original text\n",
    "    \n",
    "    '''\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s.,\\d]\", \"\", text)\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    # Create an entity label list initialized with \"O\"\n",
    "    labels = [\"O\"] * len(words)\n",
    "    \n",
    "    # Iterate through the entities to label the text\n",
    "    for entity in entities:\n",
    "        entity[\"text\"].replace(\",\",\"\")\n",
    "        entity_words = entity['text'].split()\n",
    "        category = entity['category']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            if words[i:i+len(entity_words)] == entity_words:\n",
    "                labels[i] = f\"B-{category}\"\n",
    "                for j in range(1, len(entity_words)):\n",
    "                    labels[i+j] = f\"I-{category}\"\n",
    "    \n",
    "    # Return the cleaned text and the entity labels\n",
    "    return \" \".join(words), \" \".join(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining data from the NADA dataset and NER labels into a list of JSON dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 58 column 22 (char 1130)\n",
      "Expecting ',' delimiter: line 42 column 26 (char 834)\n",
      "'category'\n",
      "Invalid control character at: line 34 column 20 (char 666)\n",
      "Expecting ',' delimiter: line 84 column 26 (char 1736)\n",
      "Expecting ',' delimiter: line 48 column 26 (char 1009)\n",
      "Expecting ',' delimiter: line 30 column 26 (char 588)\n",
      "Invalid \\escape: line 58 column 18 (char 1268)\n",
      "Expecting ',' delimiter: line 90 column 26 (char 1873)\n",
      "Expecting ',' delimiter: line 223 column 28 (char 4586)\n",
      "'entities'\n",
      "Expecting ',' delimiter: line 30 column 26 (char 573)\n",
      "Invalid control character at: line 28 column 19 (char 518)\n",
      "Expecting property name enclosed in double quotes: line 62 column 5 (char 1214)\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for idx, row in intent_data.iterrows():\n",
    "    try:\n",
    "        match = re.search(r'```json\\n(.+?)\\n```', entity_data[idx][\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"], re.DOTALL)\n",
    "        if match:\n",
    "            json_part = match.group(1)\n",
    "        entities_labels = json.loads(json_part)\n",
    "        text,entities = label_entities(row[\"text\"],entities_labels[\"entities\"])\n",
    "        intent_label = row[\"title\"]\n",
    "        # break\n",
    "        data.append({\n",
    "                \"intent_label\": intent_label,\n",
    "                \"words\": text,\n",
    "                \"word_labels\": entities,\n",
    "                \"length\": len(text)}) \n",
    "    except Exception as e: # skipping labelling errors\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting data into a dataframe\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent_label</th>\n",
       "      <th>words</th>\n",
       "      <th>word_labels</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الأدب العربي-أدبيات</td>\n",
       "      <td>يتناول المهرجان هذا العام العلاقة بين الشعر وا...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>2721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الأدب العربي-أدبيات</td>\n",
       "      <td>لاشك فى أننى لا أرمى من وراء هذا العنوان إلى أ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الأدب العربي-أدبيات</td>\n",
       "      <td>لفت الدكتور خالد جودة إلى الإتجاهات المستحدثة ...</td>\n",
       "      <td>O B-PERSON I-PERSON I-PERSON O O O O O O O O O...</td>\n",
       "      <td>2130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الأدب العربي-أدبيات</td>\n",
       "      <td>لفت الدكتور خالد جودة إلى الإتجاهات المستحدثة ...</td>\n",
       "      <td>O O B-PERSON I-PERSON O O O O O O O O O O O O ...</td>\n",
       "      <td>2130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الأدب العربي-أدبيات</td>\n",
       "      <td>أصدر فاروق حسنى وزير الثقافة المصري قرارا بتعي...</td>\n",
       "      <td>O B-PERSON I-PERSON B-ORG I-ORG I-ORG O O B-PE...</td>\n",
       "      <td>2135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7291</th>\n",
       "      <td>فلك-علوم بحتة</td>\n",
       "      <td>موقع الكون المجموعة الشمسية أقمار زحل عودة للص...</td>\n",
       "      <td>O O O O O B-GPE O O O O O O O O O O B-GPE B-PE...</td>\n",
       "      <td>1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>فلك-علوم بحتة</td>\n",
       "      <td>موقع الكون المجموعة الشمسية أقمار زحل عودة للص...</td>\n",
       "      <td>O O O O O B-GPE O O O O O O O O O O B-GPE B-PE...</td>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293</th>\n",
       "      <td>فلك-علوم بحتة</td>\n",
       "      <td>موقع الكون المجموعة الشمسية أقمار زحل عودة للص...</td>\n",
       "      <td>O B-LOC B-LOC I-LOC B-LOC B-GPE O O O O O B-LO...</td>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7294</th>\n",
       "      <td>فلك-علوم بحتة</td>\n",
       "      <td>موقع الكون المجموعة الشمسية حلقات زحل عودة للص...</td>\n",
       "      <td>O O O O O B-GPE O O O O O O O O O O B-GPE O O ...</td>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7295</th>\n",
       "      <td>فلك-علوم بحتة</td>\n",
       "      <td>موقع الكون المجموعة الشمسية حلقات زحل عودة للص...</td>\n",
       "      <td>O O B-LOC I-LOC O B-GPE O O O O O B-LOC I-LOC ...</td>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7296 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               intent_label  \\\n",
       "0     الأدب العربي-أدبيات   \n",
       "1     الأدب العربي-أدبيات   \n",
       "2     الأدب العربي-أدبيات   \n",
       "3     الأدب العربي-أدبيات   \n",
       "4     الأدب العربي-أدبيات   \n",
       "...                     ...   \n",
       "7291          فلك-علوم بحتة   \n",
       "7292          فلك-علوم بحتة   \n",
       "7293          فلك-علوم بحتة   \n",
       "7294          فلك-علوم بحتة   \n",
       "7295          فلك-علوم بحتة   \n",
       "\n",
       "                                                  words  \\\n",
       "0     يتناول المهرجان هذا العام العلاقة بين الشعر وا...   \n",
       "1     لاشك فى أننى لا أرمى من وراء هذا العنوان إلى أ...   \n",
       "2     لفت الدكتور خالد جودة إلى الإتجاهات المستحدثة ...   \n",
       "3     لفت الدكتور خالد جودة إلى الإتجاهات المستحدثة ...   \n",
       "4     أصدر فاروق حسنى وزير الثقافة المصري قرارا بتعي...   \n",
       "...                                                 ...   \n",
       "7291  موقع الكون المجموعة الشمسية أقمار زحل عودة للص...   \n",
       "7292  موقع الكون المجموعة الشمسية أقمار زحل عودة للص...   \n",
       "7293  موقع الكون المجموعة الشمسية أقمار زحل عودة للص...   \n",
       "7294  موقع الكون المجموعة الشمسية حلقات زحل عودة للص...   \n",
       "7295  موقع الكون المجموعة الشمسية حلقات زحل عودة للص...   \n",
       "\n",
       "                                            word_labels  length  \n",
       "0     O O O O O O O O O O O O O O O O O O O O O O O ...    2721  \n",
       "1     O O O O O O O O O O O O O O O O O O O O O O O ...    2380  \n",
       "2     O B-PERSON I-PERSON I-PERSON O O O O O O O O O...    2130  \n",
       "3     O O B-PERSON I-PERSON O O O O O O O O O O O O ...    2130  \n",
       "4     O B-PERSON I-PERSON B-ORG I-ORG I-ORG O O B-PE...    2135  \n",
       "...                                                 ...     ...  \n",
       "7291  O O O O O B-GPE O O O O O O O O O O B-GPE B-PE...    1311  \n",
       "7292  O O O O O B-GPE O O O O O O O O O O B-GPE B-PE...    1293  \n",
       "7293  O B-LOC B-LOC I-LOC B-LOC B-GPE O O O O O B-LO...    1293  \n",
       "7294  O O O O O B-GPE O O O O O O O O O O B-GPE O O ...    3118  \n",
       "7295  O O B-LOC I-LOC O B-GPE O O O O O B-LOC I-LOC ...    3118  \n",
       "\n",
       "[7296 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train / validation / test sets  80%/10%/10%\n",
    "df_train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "df_valid, df_test = train_test_split(temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv02\" # this value can be changed to any version of BERT (we could not use Answer.AI's ModernBERT because it's trained on English language only)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for encoding dataset\n",
    "def encode_dataset(tokenizer, text_sequences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text_sequence in text_sequences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text_sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_masks\": attention_masks}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "encoded_train = encode_dataset(tokenizer, df_train[\"words\"], max_length)\n",
    "encoded_valid = encode_dataset(tokenizer, df_valid[\"words\"], max_length)\n",
    "encoded_test = encode_dataset(tokenizer, df_test[\"words\"], max_length)\n",
    "\n",
    "intent_names = set(df[\"intent_label\"])\n",
    "intent_names = list(intent_names)\n",
    "intent_map = {label: idx for idx, label in enumerate(intent_names)}\n",
    "intent_train = torch.tensor(df_train[\"intent_label\"].map(intent_map).values)\n",
    "intent_valid = torch.tensor(df_valid[\"intent_label\"].map(intent_map).values)\n",
    "intent_test = torch.tensor(df_test[\"intent_label\"].map(intent_map).values)\n",
    "\n",
    "unique_entities = set()\n",
    "\n",
    "for sentence in df[\"word_labels\"]:\n",
    "    words = sentence.split()\n",
    "    unique_entities.update(words)\n",
    "\n",
    "unique_entities_list = list(unique_entities)\n",
    "unique_entities_list.append(\"[PAD]\")\n",
    "# Slot encoding\n",
    "slot_names = unique_entities_list\n",
    "slot_map = {label: idx for idx, label in enumerate(slot_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['الإقتصاد-علوم إجتماعية',\n",
       " 'الأدب العربي-أدبيات',\n",
       " 'عام- فنون',\n",
       " 'رياضة',\n",
       " 'فلك-علوم بحتة',\n",
       " 'القانون-علوم اجتماعية',\n",
       " 'علم الكمبيوتر-علوم بحتة',\n",
       " 'علوم صحية-علوم تطبيقية',\n",
       " 'عام-إسلام-ديانات',\n",
       " 'السياسة-علوم اجتماعية']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-GPE',\n",
       " 'B-MONEY',\n",
       " 'I-AGE',\n",
       " 'B-CARDINAL',\n",
       " 'O',\n",
       " 'B-AGE',\n",
       " 'I-PERSON',\n",
       " 'I-MISC',\n",
       " 'B-EVENT',\n",
       " 'I-ORDINAL',\n",
       " 'I-EVENT',\n",
       " 'B-ORDINAL',\n",
       " 'B-LOCATION',\n",
       " 'I-MONEY',\n",
       " 'B-MONTH',\n",
       " 'B-PERSON',\n",
       " 'I-QUANTITY',\n",
       " 'B-ORG',\n",
       " 'B-GPE',\n",
       " 'I-ORG',\n",
       " 'I-LOC',\n",
       " 'B-QUANTITY',\n",
       " 'B-NORP',\n",
       " 'I-TIME',\n",
       " 'I-CARDINAL',\n",
       " 'B-LOC',\n",
       " 'B-DATE',\n",
       " 'B-TIME',\n",
       " 'B-MISC',\n",
       " 'B-PERCENT',\n",
       " 'I-PERCENT',\n",
       " 'I-DATE',\n",
       " 'I-LOCATION',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode \n",
    "def encode_token_labels(text_sequences, slot_sequences, tokenizer, slot_map, max_length):\n",
    "    encoded_labels = torch.zeros((len(text_sequences), max_length), dtype=torch.long)\n",
    "    for i, (text, slots) in enumerate(zip(text_sequences, slot_sequences)):\n",
    "        token_labels = []\n",
    "        for word, slot in zip(text.split(), slots.split()):\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            token_labels.append(slot_map[slot])\n",
    "            expand_label = slot.replace(\"B-\", \"I-\")\n",
    "            token_labels.extend([slot_map.get(expand_label, slot_map[slot])] * (len(tokens) - 1))\n",
    "        # encoded_labels[i, 1:1 + len(token_labels)] = torch.tensor(token_labels[:max_length - 2])\n",
    "        max_tokens = min(len(token_labels), max_length - 2)\n",
    "        encoded_labels[i, 1:1 + max_tokens] = torch.tensor(token_labels[:max_tokens])\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_train = encode_token_labels(\n",
    "    df_train[\"words\"], df_train[\"word_labels\"], tokenizer, slot_map, max_length)\n",
    "slot_valid = encode_token_labels(\n",
    "    df_valid[\"words\"], df_valid[\"word_labels\"], tokenizer, slot_map, max_length)\n",
    "slot_test = encode_token_labels(\n",
    "    df_test[\"words\"], df_test[\"word_labels\"], tokenizer, slot_map, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class JointIntentAndSlotFillingModel(nn.Module):\n",
    "    def __init__(self, intent_num_labels, slot_num_labels, model_name, dropout_prob=0.1):\n",
    "        super(JointIntentAndSlotFillingModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name) # a pre-trained BERT model from Hugging Face's transformers library.\n",
    "        self.dropout = nn.Dropout(dropout_prob) # a dropout layer to prevent overfitting.\n",
    "        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, intent_num_labels) # a fully connected layer for intent classification.\n",
    "        self.slot_classifier = nn.Linear(self.bert.config.hidden_size, slot_num_labels) # a fully connected layer for slot classification.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "\n",
    "        slot_logits = self.slot_classifier(sequence_output) # logits for slot classification \n",
    "        intent_logits = self.intent_classifier(pooled_output) # Logits for intent classification \n",
    "\n",
    "        return slot_logits, intent_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = JointIntentAndSlotFillingModel(len(intent_map), len(slot_map),model_name=model_name).to(device)\n",
    "\n",
    "# optimizer and loss functions\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, eps=1e-08)\n",
    "slot_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "intent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(\n",
    "    encoded_train[\"input_ids\"], encoded_train[\"attention_masks\"], slot_train, intent_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_data = torch.utils.data.TensorDataset(\n",
    "    encoded_valid[\"input_ids\"], encoded_valid[\"attention_masks\"], slot_valid, intent_valid)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 1.2999, Train Accuracy: 0.6909 - Valid Loss: 0.8684, Valid Accuracy: 0.7466\n",
      "Epoch 2:\n",
      "  Train Loss: 0.8145, Train Accuracy: 0.7553 - Valid Loss: 0.8082, Valid Accuracy: 0.7466\n",
      "Epoch 3:\n",
      "  Train Loss: 0.7626, Train Accuracy: 0.7582 - Valid Loss: 0.7905, Valid Accuracy: 0.7315\n",
      "Epoch 4:\n",
      "  Train Loss: 0.7470, Train Accuracy: 0.7569 - Valid Loss: 0.7614, Valid Accuracy: 0.7521\n",
      "Epoch 5:\n",
      "  Train Loss: 0.7315, Train Accuracy: 0.7514 - Valid Loss: 0.7563, Valid Accuracy: 0.7466\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    intent_correct = 0\n",
    "    total_intent = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, slot_labels, intent_labels = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        slot_logits, intent_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        slot_loss = slot_loss_fn(slot_logits.view(-1, len(slot_map)), slot_labels.view(-1))\n",
    "        intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "\n",
    "        loss = slot_loss + intent_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        intent_preds = torch.argmax(intent_logits, dim=1)\n",
    "        intent_correct += (intent_preds == intent_labels).sum().item()\n",
    "        total_intent += intent_labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = intent_correct / total_intent\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    intent_correct = 0\n",
    "    total_intent = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            input_ids, attention_mask, slot_labels, intent_labels = [x.to(device) for x in batch]\n",
    "            slot_logits, intent_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            slot_loss = slot_loss_fn(slot_logits.view(-1, len(slot_map)), slot_labels.view(-1))\n",
    "            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "\n",
    "            loss = slot_loss + intent_loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # Intent accuracy\n",
    "            intent_preds = torch.argmax(intent_logits, dim=1)\n",
    "            intent_correct += (intent_preds == intent_labels).sum().item()\n",
    "            total_intent += intent_labels.size(0)\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "    valid_accuracy = intent_correct / total_intent\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test():\n",
    "    model.eval()\n",
    "\n",
    "    intent_preds_list = []\n",
    "    intent_labels_list = []\n",
    "\n",
    "    slot_preds_list = []\n",
    "    slot_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                encoded_test[\"input_ids\"],\n",
    "                encoded_test[\"attention_masks\"],\n",
    "                slot_test,\n",
    "                intent_test,\n",
    "            ),\n",
    "            batch_size=batch_size,\n",
    "        ):\n",
    "            input_ids, attention_mask, slot_labels, intent_labels = [x.to(device) for x in batch]\n",
    "            slot_logits, intent_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Intent predictions\n",
    "            intent_preds = torch.argmax(intent_logits, dim=1).cpu().numpy()\n",
    "            intent_labels = intent_labels.cpu().numpy()\n",
    "            intent_preds_list.extend(intent_preds)\n",
    "            intent_labels_list.extend(intent_labels)\n",
    "\n",
    "            # Slot predictions\n",
    "            slot_preds = torch.argmax(slot_logits, dim=2).cpu().numpy()\n",
    "            slot_labels = slot_labels.cpu().numpy()\n",
    "            for pred, label in zip(slot_preds, slot_labels):\n",
    "                slot_preds_list.extend(pred[:len(label[label > 0])])\n",
    "                slot_labels_list.extend(label[label > 0])\n",
    "\n",
    "    used_slot_names = [slot_names[i] for i in sorted(set(slot_labels_list))]\n",
    "\n",
    "    # Intent classification report\n",
    "    print(\"Intent Classification Report:\")\n",
    "    print(classification_report(intent_labels_list, intent_preds_list, target_names=intent_names))\n",
    "\n",
    "    # Slot classification report\n",
    "    print(\"Slot Classification Report:\")\n",
    "    print(classification_report(slot_labels_list, slot_preds_list, target_names=used_slot_names, labels=sorted(set(slot_labels_list))))\n",
    "\n",
    "    # Intent accuracy\n",
    "    print(f\"Intent Accuracy: {accuracy_score(intent_labels_list, intent_preds_list):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "الإقتصاد-علوم إجتماعية       1.00      0.74      0.85       144\n",
      "   الأدب العربي-أدبيات       1.00      1.00      1.00        49\n",
      "               عام- فنون       1.00      0.60      0.75        47\n",
      "                   رياضة       1.00      0.83      0.91       127\n",
      "           فلك-علوم بحتة       1.00      1.00      1.00        36\n",
      "   القانون-علوم اجتماعية       1.00      0.83      0.91       151\n",
      " علم الكمبيوتر-علوم بحتة       1.00      0.26      0.42        38\n",
      "  علوم صحية-علوم تطبيقية       1.00      1.00      1.00        39\n",
      "       عام-إسلام-ديانات       1.00      0.36      0.53        55\n",
      "   السياسة-علوم اجتماعية       0.21      1.00      0.35        44\n",
      "\n",
      "                accuracy                           0.77       730\n",
      "               macro avg       0.92      0.76      0.77       730\n",
      "            weighted avg       0.95      0.77      0.81       730\n",
      "\n",
      "Slot Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     B-MONEY       0.01      0.01      0.01       216\n",
      "       I-AGE       0.00      0.00      0.00         2\n",
      "  B-CARDINAL       0.00      0.00      0.00       295\n",
      "           O       0.94      0.93      0.94    188788\n",
      "       B-AGE       0.00      0.00      0.00         2\n",
      "    I-PERSON       0.32      0.38      0.35      2809\n",
      "      I-MISC       0.00      0.00      0.00        26\n",
      "   I-ORDINAL       0.00      0.00      0.00        59\n",
      "   B-ORDINAL       0.00      0.00      0.00        48\n",
      "     I-MONEY       0.20      0.24      0.22       425\n",
      "    B-PERSON       0.03      0.03      0.03      1727\n",
      "  I-QUANTITY       0.18      0.06      0.09       222\n",
      "       B-ORG       0.01      0.01      0.01      2791\n",
      "       B-GPE       0.07      0.07      0.07      2266\n",
      "       I-ORG       0.25      0.30      0.27      4009\n",
      "       I-LOC       0.22      0.36      0.27       820\n",
      "  B-QUANTITY       0.00      0.00      0.00        85\n",
      "      B-NORP       0.00      0.00      0.00         2\n",
      "      I-TIME       0.00      0.00      0.00        20\n",
      "  I-CARDINAL       0.00      0.00      0.00       216\n",
      "       B-LOC       0.01      0.01      0.01       481\n",
      "      B-DATE       0.01      0.01      0.01       474\n",
      "      B-TIME       0.00      0.00      0.00        17\n",
      "      B-MISC       0.00      0.00      0.00        20\n",
      "   B-PERCENT       0.00      0.00      0.00        78\n",
      "   I-PERCENT       0.17      0.15      0.16       239\n",
      "      I-DATE       0.35      0.43      0.39       543\n",
      "\n",
      "    accuracy                           0.87    206680\n",
      "   macro avg       0.10      0.11      0.11    206680\n",
      "weighted avg       0.87      0.87      0.87    206680\n",
      "\n",
      "Intent Accuracy: 0.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ubuntu/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_on_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        slot_logits, intent_logits = model(input_ids, attention_mask)\n",
    "\n",
    "    slot_ids = slot_logits.argmax(dim=-1).cpu().numpy()[0, 1:-1]\n",
    "    intent_id = intent_logits.argmax(dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    print(\"## Intent:\", intent_names[intent_id])\n",
    "    print(\"## Slots:\")\n",
    "    for token, slot_id in zip(tokenizer.tokenize(text), slot_ids):\n",
    "        print(f\"{token:>10} : {slot_names[slot_id]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Intent: القانون-علوم اجتماعية\n",
      "## Slots:\n",
      "         و : O\n",
      "       قال : O\n",
      "   الدكتور : B-PERSON\n",
      "      محمد : B-PERSON\n",
      "    الخالد : I-PERSON\n",
      "       انه : O\n",
      "      سيبا : O\n",
      "      ##شر : O\n",
      "      عمله : O\n",
      "        في : O\n",
      "    الرياض : B-GPE\n"
     ]
    }
   ],
   "source": [
    "show_predictions(\"و قال الدكتور محمد الخالد انه سيباشر عمله في الرياض\", tokenizer, model, intent_names, slot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to joint_intent_slot_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2454851/1790387467.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "save_path = \"joint_intent_slot_model.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = JointIntentAndSlotFillingModel(len(intent_map), len(slot_map),model_name=model_name)\n",
    "loaded_model.load_state_dict(torch.load(save_path))\n",
    "loaded_model.to(device)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/vocab.txt',\n",
       " './tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"./tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"./tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Intent: القانون-علوم اجتماعية\n",
      "## Slots:\n",
      "         و : O\n",
      "       قال : O\n",
      "   الدكتور : B-PERSON\n",
      "      محمد : B-PERSON\n",
      "    الخالد : I-PERSON\n",
      "       انه : O\n",
      "      سيبا : O\n",
      "      ##شر : O\n",
      "      عمله : O\n",
      "        في : O\n",
      "    الرياض : B-GPE\n"
     ]
    }
   ],
   "source": [
    "show_predictions(\"و قال الدكتور محمد الخالد انه سيباشر عمله في الرياض\", loaded_tokenizer, loaded_model, intent_names, slot_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text, tokenizer, intent_names, slot_names,\n",
    "                       intent_id, slot_ids):\n",
    "    info = {\"intent\": intent_names[intent_id]}\n",
    "    collected_slots = {}\n",
    "    active_slot_words = []\n",
    "    active_slot_name = None\n",
    "    for word in text.split():\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        current_word_slot_ids = slot_ids[:len(tokens)]\n",
    "        slot_ids = slot_ids[len(tokens):]\n",
    "        current_word_slot_name = slot_names[current_word_slot_ids[0]]\n",
    "        if current_word_slot_name == \"O\":\n",
    "            if active_slot_name:\n",
    "                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n",
    "                active_slot_words = []\n",
    "                active_slot_name = None\n",
    "        else:\n",
    "            # Naive BIO: handling: treat B- and I- the same...\n",
    "            new_slot_name = current_word_slot_name[2:]\n",
    "            if active_slot_name is None:\n",
    "                active_slot_words.append(word)\n",
    "                active_slot_name = new_slot_name\n",
    "            elif new_slot_name == active_slot_name:\n",
    "                active_slot_words.append(word)\n",
    "            else:\n",
    "                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n",
    "                active_slot_words = [word]\n",
    "                active_slot_name = new_slot_name\n",
    "    if active_slot_name:\n",
    "        collected_slots[active_slot_name] = \" \".join(active_slot_words)\n",
    "    info[\"slots\"] = collected_slots\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'القانون-علوم اجتماعية',\n",
       " 'slots': {'PERSON': 'الدكتور محمد الخالد', 'GPE': 'الرياض'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        slot_logits, intent_logits = model(input_ids, attention_mask)\n",
    "\n",
    "    slot_ids = slot_logits.argmax(dim=-1).cpu().numpy()[0, 1:-1]\n",
    "    intent_id = intent_logits.argmax(dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    return decode_predictions(text, tokenizer, intent_names, slot_names,\n",
    "                              intent_id, slot_ids)\n",
    "\n",
    "nlu(\"و قال الدكتور محمد الخالد انه سيباشر عمله في الرياض\",\n",
    "    loaded_tokenizer, loaded_model, intent_names, slot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
